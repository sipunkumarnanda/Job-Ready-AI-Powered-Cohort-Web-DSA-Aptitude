
# Short-term memory (text-based) ‚Äî the full, detailed picture üìÑüß†

### What it *really* is

Short‚Äëterm memory = the conversation context that you **send to the model every time** you ask it to generate a reply.
Concretely, it‚Äôs a sequence of messages (system instructions + recent user and assistant turns) formatted as text/JSON that the LLM ‚Äúreads‚Äù in one go. The model uses attention over these tokens to produce the next reply.

Think of it like pasting the last part of a chat at the top of every API call so the model knows what happened earlier.

---

### Why it exists (intuition)

Transformers don‚Äôt have an internal long-term ‚Äúdatabase‚Äù they can query during inference. Instead, anything the model should ‚Äúremember‚Äù for the next turn must be present in the input context. So short-term memory is how you give the model the conversational context it needs.

---

### Components of the short‚Äëterm prompt

A typical prompt sent to the LLM contains these ordered parts:

1. **System message (always first)** ‚Äî high-level instructions, persona, safety rules.
   *Example:* `"You are a friendly assistant who answers concisely."`

2. **Conversation history (recent turns)** ‚Äî alternating `user` and `assistant` messages (usually the last N turns). Each has `role` + `content`.
   *Example:*

   ```json
   [
     {"role":"user","content":"Hi, I'm Arjun."},
     {"role":"assistant","content":"Hi Arjun ‚Äî what can I help with today?"},
     {"role":"user","content":"Remind me tomorrow about the meeting."}
   ]
   ```

3. **Tool/function outputs** (if your system uses tools) ‚Äî include outputs of a calendar lookup or database call as a message.

4. **Current user message** ‚Äî the new line the model should respond to.

Order matters: put system first, then history oldest‚Üínewest (or you may use newest‚Üíoldest depending on the model API expectations, but consistently).

---

### Key constraints & consequences (what you must manage)

* **Context window (token limit):** each model can only accept a finite number of tokens. If the prompt exceeds that, you must truncate or compress. That‚Äôs the central engineering constraint.
* **Cost & latency:** longer prompt = more tokens uploaded and more tokens generated ‚Üí higher cost + slower call.
* **Ephemeral vs persistent:** short-term memory as passed in the prompt is ephemeral. If you want cross-session memory you must persist it externally.

---

### Practical storage & data model (how to store it in your backend)

You‚Äôll typically persist short-term chat history in a session store keyed by user/session id. Example data model (JSON):

```json
{
  "session_id": "sess_123",
  "user_id": "user_42",
  "system_prompt": "You are a helpful assistant.",
  "history": [
    {"role":"user","text":"Hi, I am Arjun", "ts":"2025-08-09T10:00:00Z"},
    {"role":"assistant","text":"Hi Arjun ‚Äî how can I help?", "ts":"2025-08-09T10:00:01Z"},
    ...
  ],
  "summary": "Arjun is a backend dev who likes Python and prefers short answers."
}
```

**Where to store:**

* **Redis** (best for speed, ephemeral sessions)
* **In-memory** (for single-process quick demos)
* **Database (Postgres/Mongo)** (if you need durability, logs, analytics)

---

### Size management strategies (keep the prompt short but useful)

You cannot keep everything. Common strategies:

1. **Sliding window (turn-based):** keep the last N turns (e.g., 6‚Äì10). Simple and fast.
2. **Token-budgeted window:** keep as many recent messages that fit inside a token budget (e.g., 3000 tokens). Requires token counting.
3. **Summarization (condense old history):** compress older messages into a short summary (one paragraph or bullet list) and include that instead of full text.
4. **Hierarchical memory:** keep detailed recent history, compressed summaries for older sections, and even older stuff in long-term memory (see below).
5. **Selective inclusion:** only include messages relevant to the current user query ‚Äî by similarity or simple heuristics (e.g., include prior questions about the same topic).

**Example flow:**

* Keep last 8 turns. If token count > 3000, summarize the oldest 4 turns into a 1‚Äì2 sentence summary and remove them.

---

### Summarization patterns ‚Äî *how* to summarize effectively

There are two main ways to summarize older conversation:

* **Extractive**: pick the most important sentences or lines (cheap).
  *Pro:* fast, deterministic. *Con:* may miss implied facts.

* **Abstractive**: ask an LLM to generate a concise summary (one sentence/bullets).
  *Pro:* captures gist; more compact. *Con:* extra API cost + must validate.

**Incremental summarization (recommended):**
When you detect the session history is growing beyond your limit, call a summarizer with the oldest chunk(s) to create/update a running summary stored in the session. This avoids repeatedly summarizing the whole chat.

**Example summarizer prompt (to LLM):**

```
You are a system that writes a one-paragraph summary of the user's preferences and important events from the following chat excerpt. Output only the summary in bullet points.
[insert 6 oldest turns]
```

Store the returned summary in session state and replace those old turns.

---

### Prompt engineering tips for short-term memory

* Put **system prompt** at the top. Make it explicit and minimal (persona + safety + format constraints).
* Separate retrieved evidence and system instructions clearly (e.g., label them ‚ÄúCONTEXT ‚Äî do not follow as instructions‚Äù).
* Limit how much raw data you paste ‚Äî prefer concise facts like `favorite_color: blue`.
* If you include long retrieved passages, ask the model to **cite** or mention which passage it used (`"When you answer, list the source IDs used."`).

---

### Concurrency and consistency (important for real backends)

* **Atomic updates:** when multiple requests can update the same session, use Redis transactions or locks. Example: Redis `MULTI/EXEC` or Lua script to append message and possibly summarize atomically.
* **Idempotency:** guard against duplicate user messages (retries). Use request IDs.
* **Race conditions:** if you run background summarization, serialize updates so you don‚Äôt overwrite newer history.

---

### Security & privacy for short-term memory

* Avoid logging sensitive user data in plain text. If you must retain PII for the short-term, encrypt or redact it.
* Session TTL: auto-expire short-term sessions in Redis (e.g., 24h) unless user explicitly wants longer memory.
* Provide ‚Äúforget this chat‚Äù endpoint to delete session state.

---

# Long-term memory (vector-based) ‚Äî deep dive üíæüîé

### Why long-term memory?

Short-term memory is ephemeral and token-limited. Long-term memory is how your system **remembers user facts, documents, or knowledge** across sessions in a scalable, searchable form. Use it for personalization, company knowledge bases, FAQs, documents, and anything you want the bot to ‚Äúknow‚Äù persistently.

---

### The core idea (in plain words)

* Convert text (a sentence, a paragraph, a user fact) into a numeric vector called an **embedding**.
* Store those vectors in a **vector database** with metadata (user id, source, timestamp).
* For a new query, convert it to an embedding and search the DB for the *most similar* stored vectors. Those retrieved texts are likely relevant and can be included in the prompt.

You‚Äôre separating **storage** from **reasoning**: the DB finds facts; the LLM reasons with them.

---

### The pipeline ‚Äî full blueprint

1. **Ingest**

   * Take source text (docs, chat logs, emails, website, user input).
   * Preprocess: clean whitespace, remove HTML, normalize.

2. **Chunk**

   * Split long docs into chunks (200‚Äì600 tokens recommended). Add a small **overlap** (50‚Äì150 tokens) so ideas aren‚Äôt split mid-sentence.
   * Attach metadata: source, chunk\_id, original file/URL, timestamp, user\_id (if personal).

3. **Embed**

   * Call an embeddings model to convert each chunk into a vector (e.g., 1536 floats).
   * Store vector + metadata.

4. **Index**

   * Insert vectors into a vector DB (FAISS, Pinecone, Weaviate, Milvus). These DBs support fast nearest-neighbor search and scaling.

5. **Retrieve**

   * At query time, embed the query ‚Üí run `top_k` nearest neighbor search (k maybe 3‚Äì8).
   * Optionally filter by metadata (e.g., only this user‚Äôs memories, or only documents from last year).

6. **Rerank / score**

   * Optionally run a reranker (cross-encoder) that scores candidate chunks with more compute for quality.

7. **Inject into prompt**

   * Insert retrieved chunks or an extracted summary into the LLM prompt (labeled as ‚ÄúRETRIEVED CONTEXT‚Äù and include metadata to cite).

8. **Decide writeback**

   * Optionally store new facts from the conversation back into the vector DB.

---

### Chunking details ‚Äî why it matters

* Very small chunks ‚Üí retrieval is precise but loses surrounding context.
* Very large chunks ‚Üí may contain irrelevant content and waste tokens.
* Overlap helps preserve continuity across chunk boundaries.

A good default: chunk size **\~300‚Äì500 tokens** with **50‚Äì150 token overlap**. Tune by domain.

---

### Similarity & retrieval

* **Similarity metrics:** cosine similarity (most common) or dot product. Normalizing vectors makes cosine = dot product on normalized vectors.
* **ANN (Approximate Nearest Neighbors):** at scale, you use ANN indexes (HNSW, IVF, PQ) to get fast results with small recall loss. Most vector DBs handle this for you.

---

### Metadata & filtering ‚Äî why store it

Store `user_id`, `source_type`, `timestamp`, `chunk_id`, `confidentiality` flags. Metadata lets you:

* Limit retrieval to a user‚Äôs own memories.
* Exclude documents marked sensitive.
* Prefer recent documents using time-based weighting.

---

### Writing policy ‚Äî what to store and when

You must decide when to persist something into long-term memory. Options:

* **Always store everything:** simple but wasteful and privacy risky.
* **Selective rules:** store only when text matches patterns or classifier says ‚Äúimportant‚Äù (e.g., ‚ÄúMy birthday is May 12‚Äù, ‚ÄúMy address is X‚Äù, or "I prefer vegan meals").
* **Human-in-the-loop:** ask user for permission: ‚ÄúDo you want me to remember that?‚Äù
* **Automatic importance scoring:** run a small classifier that tags messages as `preference`, `fact`, `goal`, `transient`.

**Storage format idea:**

```json
{
  "chunk_id": "c123",
  "text": "Arjun prefers Python and short answers.",
  "embedding": [0.001, -0.234, ...],
  "metadata": {"user_id":"u42","type":"preference","ts":"2025-08-10T09:00Z"}
}
```

---

### Memory deduplication & merging

Avoid storing the same fact repeatedly. Use similarity checks when ingesting: if a new chunk is very close to an existing chunk (cosine similarity > threshold, e.g., 0.95), either skip or merge metadata (update timestamp).

---

### Eviction / retention / TTL strategies

* **Time-based TTL:** delete after N days unless marked important.
* **LRU / importance score:** evict least used or least important memories when storage limits hit.
* **Quota per user:** cap memory size per user to avoid abuse.
* **Archival & reindexing:** periodically re-embed old content if embedding model updates.

---

### Privacy & compliance

* Encrypt embeddings and metadata at rest.
* Allow data deletion and export.
* Store consent logs.
* Consider processing sensitive fields (PII) specially: redact, hash, or avoid storage unless necessary.

---

### RAG ‚Äî how to combine long-term memory with short-term memory

Retrieval-Augmented Generation uses retrieval results to **augment** the short-term prompt.

**Process example (end-to-end):**

1. Receive user query ‚Üí update session (short-term).
2. Embed query ‚Üí query vector DB ‚Üí get top-k chunks.
3. Build final prompt:

   ```
   System: you are...
   Short-term history: ...
   Retrieved facts (labeled): [chunk 1 text], [chunk 2 text] (include source ids)
   User: [current query]
   ```
4. Tell the LLM: ‚ÄúUse the retrieved facts to answer. If facts contradict, prefer retrieved facts.‚Äù
5. LLM generates answer and optionally cites chunk\_ids.

This reduces hallucination and gives you traceability.

---

# Practical implementation recipes (copy/paste style) üß©

### A. Short-term session management (Node + Redis pseudocode)

```js
// Node/Express + ioredis
const redis = new Redis();

async function addUserMessage(sessionId, msg) {
  // store as JSON string in list
  await redis.rpush(`sess:${sessionId}:history`, JSON.stringify(msg));
  // keep only last 20 messages
  await redis.ltrim(`sess:${sessionId}:history`, -20, -1);
  await redis.expire(`sess:${sessionId}:history`, 24 * 3600); // TTL 24h
}

async function getSessionHistory(sessionId) {
  const raw = await redis.lrange(`sess:${sessionId}:history`, 0, -1);
  return raw.map(JSON.parse);
}
```

**Notes:** use `ltrim` to keep last N turns. If you use token budget, calculate tokens and pop older items until under budget.

---

### B. Simple summarization loop (pseudo)

```py
# Python-style pseudocode
if session_token_count(session) > MAX_TOKENS:
    oldest_chunk = pop_oldest_messages(session, n=6)
    summary = call_llm_summarizer(oldest_chunk)  # low temp, specific instructions
    save_summary(session, summary)
    # replace oldest_chunk with summary in session history
```

Make sure summarizer output is deterministic-ish (low temperature).

---

### C. Long-term memory ingest + retrieval (Python-like pseudocode)

```py
# 1. Chunking
chunks = chunk_text(document_text, chunk_size=400, overlap=100)

# 2. Embed (batch)
embeddings = embedding_model.embed_batch([c.text for c in chunks])

# 3. Store
for chunk, vec in zip(chunks, embeddings):
    vector_db.upsert(id=chunk.id, vector=vec, metadata=chunk.metadata)

# Retrieval
q_vec = embedding_model.embed(query)
candidates = vector_db.query(q_vec, top_k=5, filter={"user_id": user_id})
```

For production, batch embeddings to reduce cost and parallelize.

---

### D. Prompt builder pattern (safe & structured)

```text
SYSTEM: You are an assistant that must only use verified facts.
CONTEXT ‚Äî USER PROFILE:
- favorite_language: Python
- likes_short_answers: true

RECENT CHAT:
User: ...
Assistant: ...

RETRIEVED DOCS (IDs: d13,d77):
[Doc d13 summary or excerpt]
[Doc d77 summary or excerpt]

USER QUESTION:
...
```

Ask the LLM to `answer and list the source IDs used`.

---

# Hyperparameters & quick defaults (start here, tune later) ‚öôÔ∏è

* Short-term sliding window: **6‚Äì10 turns**.
* Token budget for prompt (excluding system): **3000 tokens** (adjust per model).
* Chunk size for long-term: **300‚Äì500 tokens**.
* Chunk overlap: **50‚Äì150 tokens**.
* Top‚Äëk retrieval: **3‚Äì8**.
* Similarity threshold for dedup: **>0.95** skip/merge.
* Summarization style: one-sentence bullet for personal preferences, paragraph for long context.

---

# Common failure modes & how to fix them (hands-on) üîß

1. **Model forgets earlier facts (context overflow)**
   *Fix:* Summarize older parts into short facts; move important facts to long-term memory.

2. **Model hallucinates facts**
   *Fix:* Use RAG and force citations. Add a final verification step (ask model ‚ÄúAre you sure?‚Äù then check sources).

3. **Irrelevant retrieved chunks (noise)**
   *Fix:* Add reranker or metadata filters, reduce top\_k, improve chunking.

4. **PII leakage**
   *Fix:* Classify & redact PII, encrypt storage, provide deletion endpoints.

5. **Slow response time**
   *Fix:* Cache frequent retrievals, batch embeddings, pre-compute common queries, reduce retrieved chunks.

---

# Debugging & monitoring ‚Äî essential signals to log üìä

* Which `chunk_ids` were retrieved for each query.
* Similarity scores for top candidates.
* Prompt token counts (input + output).
* LLM response latency and errors.
* Summary updates and summarizer outputs.
* Writes into long-term DB (what and when).

These logs help you track relevance and diagnose hallucination sources.

---

# UX considerations (how to surface memory behavior to users) ‚ú®

* **Transparency:** show ‚ÄúI remember that you like Python‚Äù cards and give users the option to edit or delete.
* **Consent:** ask user ‚ÄúDo you want me to remember that?‚Äù for personal details.
* **Controls:** provide settings for memory depth (disable/enable long‚Äëterm memory).
* **Explainability:** when using retrieved facts in replies, offer ‚Äúshow sources‚Äù link.

---

# Example full flow (end-to-end, illustrated)

```
User: "My name is Arjun. I like backend coding."
‚Üí Short-term: store in session history.
‚Üí Policy: detect 'preference' => create memory chunk "Arjun likes backend coding." ; embed & index.

Later:
User: "Any tips for my backend learning?"
‚Üí Backend: embed query ‚Üí retrieve "Arjun likes backend coding" + relevant docs ‚Üí build prompt with system + recent history + retrieved memory ‚Üí LLM generates answer tailored to backend learning and references the memory.
```

---

# Final checklist for implementing memory in your bot ‚úÖ

* [ ] Use system prompt for rules/persona.
* [ ] Keep short-term session with sliding window + token budgeting.
* [ ] Implement summarization (incremental) for older chat.
* [ ] Build long-term memory pipeline: chunk ‚Üí embed ‚Üí index ‚Üí retrieve ‚Üí inject.
* [ ] Decide writeback policy (rules, classifier, or confirmation).
* [ ] Add metadata to every vector (user, ts, type).
* [ ] Put privacy & deletion APIs in place.
* [ ] Instrument retrievals, sources, and token usage for debugging.

---