
# 🔎 Querying Chat Memory from Pinecone — Deep Dive

You’ve now added the **retrieval part** of your memory system:

1. **`vector.service.js`** → defines a helper function `queryMemory()` to search Pinecone for similar past messages.
2. **`socket.server.js`** → calls `queryMemory()` whenever you want to fetch related memories for a new message.

👉 This completes the **RAG loop**:

* First, you **save** embeddings with `createMemory()`.
* Then, you **search** embeddings with `queryMemory()` to find relevant past context.

---

## 1️⃣ `vector.service.js` — Querying Memory 🧠

```js
// vector.service.js
async function queryMemory({ queryVector, limit = 5, metadata }) {
  const data = await cohortChatGptIndex.query({
    vector: queryVector,
    topK: limit,
    filter: metadata ? { metadata } : undefined,
    includeMetadata: true
  });

  return data.matches;
}
```

### 🔍 Explanation:

* **`queryMemory()`** → async function that looks up Pinecone for vectors similar to `queryVector`.

* **Parameters:**

  * `queryVector` → the embedding of the query/message you want to compare.
  * `limit` → how many top matches to return (default = 5).
  * `metadata` → optional filter (only return matches with certain metadata).

* **Inside:**

  * Calls Pinecone `index.query()` with:

    * `vector: queryVector` → the vector to search against.
    * `topK: limit` → number of most similar results.
    * `filter: metadata ? {metadata} : undefined` → applies filtering if provided.
    * `includeMetadata: true` → ensures metadata is returned with results.

* **Returns:** `data.matches` → array of matching vectors + metadata.

---

### ✅ Example Call:

```js
const results = await queryMemory({
  queryVector: [0.12, 0.56, 0.87, ...],
  limit: 3,
  metadata: { chat: "chat-room-123" }
});

console.log(results);
```

This might return:

```json
[
  {
    "id": "msg-67890",
    "score": 0.92,
    "metadata": {
      "chat": "chat-room-123",
      "user": "user-99",
      "text": "Tell me about vector databases."
    }
  },
  {
    "id": "msg-12345",
    "score": 0.88,
    "metadata": {
      "chat": "chat-room-123",
      "user": "user-42",
      "text": "What is Pinecone?"
    }
  }
]
```

---

## 2️⃣ `socket.server.js` — Using `queryMemory()` 💬

```js
// socket.server.js
const memory = await queryMemory({
  queryVector: vectors,
  limit: 3,
  metadata: {}
});

console.log("Memory : ", memory);
```

### 🔍 Explanation:

* Here, `queryMemory()` is being called when a new message is processed.

* **Parameters:**

  * `queryVector: vectors` → the embedding of the incoming message.
  * `limit: 3` → fetch top 3 similar past messages.
  * `metadata: {}` → empty, meaning no filtering (search across all stored vectors).

* After fetching, results are logged with `console.log("Memory : ", memory)`.

---

### ✅ Example Console Output:

```
Memory : [
  { id: 'msg-101', score: 0.91, metadata: { chat: 'chat-room-1', user: 'u123', text: 'Let’s talk about RAG.' } },
  { id: 'msg-102', score: 0.87, metadata: { chat: 'chat-room-1', user: 'u456', text: 'Pinecone stores embeddings.' } },
  { id: 'msg-103', score: 0.85, metadata: { chat: 'chat-room-1', user: 'u789', text: 'How do we query vectors?' } }
]
```

---

## 3️⃣ Why This is Powerful ⚡

* 🧠 **Semantic recall** → Your app can “remember” past conversations by meaning, not just keywords.
* 🔎 **Context for LLMs** → Query results can be passed into Gemini/OpenAI as context for better answers.
* 🧵 **Conversation threading** → Limit results by `chatId` so each chat session retrieves only its own history.
* 🎯 **Personalization** → You could filter by `user` to only retrieve that user’s past data.

---

## 4️⃣ Big Picture Workflow 🏗️

1. **User sends new message** → embed it into a vector.
2. **Store in Pinecone** with `createMemory()`.
3. **Query Pinecone** with `queryMemory()` to fetch top-k similar messages.
4. **Send matches as context** → pass into Gemini for RAG responses.

---

## ✨ Analogy

Imagine Pinecone is a **searchable brain**:

* `createMemory()` = storing new memories in the brain.
* `queryMemory()` = asking the brain *“What do you remember that’s similar to this?”*

---

⚡ In short:

* `vector.service.js → queryMemory()` = function that **searches Pinecone for similar past messages**.
* `socket.server.js` = demonstrates how to **fetch relevant context** for each new user message.
* Together, they let your chatbot **recall conversation history intelligently**.

---