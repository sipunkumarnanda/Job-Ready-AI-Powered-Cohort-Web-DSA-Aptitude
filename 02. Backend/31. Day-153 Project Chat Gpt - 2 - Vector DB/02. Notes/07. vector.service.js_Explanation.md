
# 📘 Pinecone Vector Service Notes

This file (`vector.service.js`) is your **service layer** for handling memory storage and retrieval with **Pinecone**. It provides two main functions:

1. ✍️ **`createMemory`** → Store a new memory (message embedding + metadata) in Pinecone.
2. 🔎 **`queryMemory`** → Retrieve past memories from Pinecone based on similarity search.

---

## 🧾 Full Code

```js
// Import the Pinecone library
import { Pinecone } from '@pinecone-database/pinecone'

// Initialize a Pinecone client with your API key
const pc = new Pinecone({ apiKey: process.env.PINECONE_APIKEY });

const cohortChatGptIndex = pc.Index('cohort-chat-gpt')

async function createMemory({vectors, messageId, metadata}){
     await cohortChatGptIndex.upsert([{
        id : messageId ,
        values : vectors,
        metadata
     }])
}

async function queryMemory({queryVector, limit = 5 , metadata}) {
    const data = await cohortChatGptIndex.query({
        vector : queryVector,
        topK : limit,
        filter : metadata ? {metadata} : undefined ,
        includeMetadata : true
    })

    return data.matches
}

export  { createMemory, queryMemory }
```

---

## 🏗️ Block-by-Block Explanation

### 1️⃣ Import & Client Setup

```js
import { Pinecone } from '@pinecone-database/pinecone'

const pc = new Pinecone({ apiKey: process.env.PINECONE_APIKEY });
```

* 📦 **Import Pinecone SDK** → lets you interact with Pinecone.
* 🔑 **API key** → pulled securely from `.env`.
* 🛠️ **Client instance (`pc`)** → acts like a connection handle to Pinecone.

Think of this like **connecting to a database** before you can run queries.

---

### 2️⃣ Define the Index

```js
const cohortChatGptIndex = pc.Index('cohort-chat-gpt')
```

* This connects you to the **`cohort-chat-gpt` index** in Pinecone.
* 🗂️ An **index** = a searchable collection of vectors.
* All memory operations (`upsert`, `query`) will run against this index.

---

### 3️⃣ `createMemory` Function ✍️

```js
async function createMemory({vectors, messageId, metadata}){
     await cohortChatGptIndex.upsert([{
        id : messageId ,
        values : vectors,
        metadata
     }])
}
```

#### 🔑 What it does:

* **Upserts** (insert/update) a new memory into Pinecone.
* Accepts:

  * `vectors` → the embedding (array of floats, e.g. `[0.12, 0.34, ...]`)
  * `messageId` → unique identifier for the memory (so it can be updated later)
  * `metadata` → JSON with extra context (`chatId`, `userId`, `text`, etc.)

#### Example:

```js
await createMemory({
  vectors: [0.1, 0.2, 0.3, ...],
  messageId: "msg-101",
  metadata: { chat: "chat-1", user: "u123", text: "Hey there!" }
})
```

👉 Pinecone now stores this embedding and links it to your metadata.
👉 If `msg-101` already exists, Pinecone **updates** it.

---

### 4️⃣ `queryMemory` Function 🔎

```js
async function queryMemory({queryVector, limit = 5 , metadata}) {
    const data = await cohortChatGptIndex.query({
        vector : queryVector,
        topK : limit,
        filter : metadata ? {metadata} : undefined ,
        includeMetadata : true
    })

    return data.matches
}
```

#### 🔑 What it does:

* Performs a **similarity search** in Pinecone.
* Accepts:

  * `queryVector` → the embedding of the search query.
  * `limit` → number of closest matches to return (`topK`).
  * `metadata` → optional filter (e.g., search only inside a certain chat).

#### Key Options:

* `vector: queryVector` → the embedding you’re comparing against.
* `topK: limit` → how many matches to return.
* `filter` → restrict results (only from certain `chatId` or `user`).
* `includeMetadata: true` → ensures you also get stored metadata (`text`, `chat`, `user`) in the results.

#### Example:

```js
const results = await queryMemory({
  queryVector: [0.1, 0.2, 0.3, ...],
  limit: 3,
  metadata: { chat: { $eq: "chat-1" } }
})

console.log(results)
```

👉 Returns up to 3 closest memories **from chat-1**, including their metadata.

---

### 5️⃣ Exporting Functions 🚪

```js
export { createMemory, queryMemory }
```

* Makes both functions reusable in other files (like your `socket.server.js`).
* Typical usage:

```js
import { createMemory, queryMemory } from './vector.service.js'
```

---

## 🧠 How It Fits in Your App

* 📨 When a **user sends a message** → you create an embedding → call `createMemory()` → store it in Pinecone.
* 🔎 When you need **context for a new query** → you embed the query → call `queryMemory()` → fetch past relevant messages.

This is the foundation of a **memory system** for your chatbot (like ChatGPT with memory).

---

## ✨ Analogy

Think of Pinecone as a **giant smart notebook**:

* ✍️ `createMemory` = writing a note into the notebook (with labels).
* 🔎 `queryMemory` = flipping through the notebook to find notes most related to your current question.

---

✅ With this service, your chatbot can **remember past chats**, **search by meaning**, and **provide context-aware responses**.

---