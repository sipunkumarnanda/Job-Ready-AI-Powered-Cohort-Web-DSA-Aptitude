
# Short-term memory (text-based) â€” the full, detailed picture ðŸ“„ðŸ§ 

### What it *really* is

Shortâ€‘term memory = the conversation context that you **send to the model every time** you ask it to generate a reply.
Concretely, itâ€™s a sequence of messages (system instructions + recent user and assistant turns) formatted as text/JSON that the LLM â€œreadsâ€ in one go. The model uses attention over these tokens to produce the next reply.

Think of it like pasting the last part of a chat at the top of every API call so the model knows what happened earlier.

---

### Why it exists (intuition)

Transformers donâ€™t have an internal long-term â€œdatabaseâ€ they can query during inference. Instead, anything the model should â€œrememberâ€ for the next turn must be present in the input context. So short-term memory is how you give the model the conversational context it needs.

---

### Components of the shortâ€‘term prompt

A typical prompt sent to the LLM contains these ordered parts:

1. **System message (always first)** â€” high-level instructions, persona, safety rules.
   *Example:* `"You are a friendly assistant who answers concisely."`

2. **Conversation history (recent turns)** â€” alternating `user` and `assistant` messages (usually the last N turns). Each has `role` + `content`.
   *Example:*

   ```json
   [
     {"role":"user","content":"Hi, I'm Arjun."},
     {"role":"assistant","content":"Hi Arjun â€” what can I help with today?"},
     {"role":"user","content":"Remind me tomorrow about the meeting."}
   ]
   ```

3. **Tool/function outputs** (if your system uses tools) â€” include outputs of a calendar lookup or database call as a message.

4. **Current user message** â€” the new line the model should respond to.

Order matters: put system first, then history oldestâ†’newest (or you may use newestâ†’oldest depending on the model API expectations, but consistently).

---

### Key constraints & consequences (what you must manage)

* **Context window (token limit):** each model can only accept a finite number of tokens. If the prompt exceeds that, you must truncate or compress. Thatâ€™s the central engineering constraint.
* **Cost & latency:** longer prompt = more tokens uploaded and more tokens generated â†’ higher cost + slower call.
* **Ephemeral vs persistent:** short-term memory as passed in the prompt is ephemeral. If you want cross-session memory you must persist it externally.

---

### Practical storage & data model (how to store it in your backend)

Youâ€™ll typically persist short-term chat history in a session store keyed by user/session id. Example data model (JSON):

```json
{
  "session_id": "sess_123",
  "user_id": "user_42",
  "system_prompt": "You are a helpful assistant.",
  "history": [
    {"role":"user","text":"Hi, I am Arjun", "ts":"2025-08-09T10:00:00Z"},
    {"role":"assistant","text":"Hi Arjun â€” how can I help?", "ts":"2025-08-09T10:00:01Z"},
    ...
  ],
  "summary": "Arjun is a backend dev who likes Python and prefers short answers."
}
```

**Where to store:**

* **Redis** (best for speed, ephemeral sessions)
* **In-memory** (for single-process quick demos)
* **Database (Postgres/Mongo)** (if you need durability, logs, analytics)

---

### Size management strategies (keep the prompt short but useful)

You cannot keep everything. Common strategies:

1. **Sliding window (turn-based):** keep the last N turns (e.g., 6â€“10). Simple and fast.
2. **Token-budgeted window:** keep as many recent messages that fit inside a token budget (e.g., 3000 tokens). Requires token counting.
3. **Summarization (condense old history):** compress older messages into a short summary (one paragraph or bullet list) and include that instead of full text.
4. **Hierarchical memory:** keep detailed recent history, compressed summaries for older sections, and even older stuff in long-term memory (see below).
5. **Selective inclusion:** only include messages relevant to the current user query â€” by similarity or simple heuristics (e.g., include prior questions about the same topic).

**Example flow:**

* Keep last 8 turns. If token count > 3000, summarize the oldest 4 turns into a 1â€“2 sentence summary and remove them.

---

### Summarization patterns â€” *how* to summarize effectively

There are two main ways to summarize older conversation:

* **Extractive**: pick the most important sentences or lines (cheap).
  *Pro:* fast, deterministic. *Con:* may miss implied facts.

* **Abstractive**: ask an LLM to generate a concise summary (one sentence/bullets).
  *Pro:* captures gist; more compact. *Con:* extra API cost + must validate.

**Incremental summarization (recommended):**
When you detect the session history is growing beyond your limit, call a summarizer with the oldest chunk(s) to create/update a running summary stored in the session. This avoids repeatedly summarizing the whole chat.

**Example summarizer prompt (to LLM):**

```
You are a system that writes a one-paragraph summary of the user's preferences and important events from the following chat excerpt. Output only the summary in bullet points.
[insert 6 oldest turns]
```

Store the returned summary in session state and replace those old turns.

---

### Prompt engineering tips for short-term memory

* Put **system prompt** at the top. Make it explicit and minimal (persona + safety + format constraints).
* Separate retrieved evidence and system instructions clearly (e.g., label them â€œCONTEXT â€” do not follow as instructionsâ€).
* Limit how much raw data you paste â€” prefer concise facts like `favorite_color: blue`.
* If you include long retrieved passages, ask the model to **cite** or mention which passage it used (`"When you answer, list the source IDs used."`).

---

### Concurrency and consistency (important for real backends)

* **Atomic updates:** when multiple requests can update the same session, use Redis transactions or locks. Example: Redis `MULTI/EXEC` or Lua script to append message and possibly summarize atomically.
* **Idempotency:** guard against duplicate user messages (retries). Use request IDs.
* **Race conditions:** if you run background summarization, serialize updates so you donâ€™t overwrite newer history.

---

### Security & privacy for short-term memory

* Avoid logging sensitive user data in plain text. If you must retain PII for the short-term, encrypt or redact it.
* Session TTL: auto-expire short-term sessions in Redis (e.g., 24h) unless user explicitly wants longer memory.
* Provide â€œforget this chatâ€ endpoint to delete session state.

---

# Long-term memory (vector-based) â€” deep dive ðŸ’¾ðŸ”Ž

### Why long-term memory?

Short-term memory is ephemeral and token-limited. Long-term memory is how your system **remembers user facts, documents, or knowledge** across sessions in a scalable, searchable form. Use it for personalization, company knowledge bases, FAQs, documents, and anything you want the bot to â€œknowâ€ persistently.

---

### The core idea (in plain words)

* Convert text (a sentence, a paragraph, a user fact) into a numeric vector called an **embedding**.
* Store those vectors in a **vector database** with metadata (user id, source, timestamp).
* For a new query, convert it to an embedding and search the DB for the *most similar* stored vectors. Those retrieved texts are likely relevant and can be included in the prompt.

Youâ€™re separating **storage** from **reasoning**: the DB finds facts; the LLM reasons with them.

---

### The pipeline â€” full blueprint

1. **Ingest**

   * Take source text (docs, chat logs, emails, website, user input).
   * Preprocess: clean whitespace, remove HTML, normalize.

2. **Chunk**

   * Split long docs into chunks (200â€“600 tokens recommended). Add a small **overlap** (50â€“150 tokens) so ideas arenâ€™t split mid-sentence.
   * Attach metadata: source, chunk\_id, original file/URL, timestamp, user\_id (if personal).

3. **Embed**

   * Call an embeddings model to convert each chunk into a vector (e.g., 1536 floats).
   * Store vector + metadata.

4. **Index**

   * Insert vectors into a vector DB (FAISS, Pinecone, Weaviate, Milvus). These DBs support fast nearest-neighbor search and scaling.

5. **Retrieve**

   * At query time, embed the query â†’ run `top_k` nearest neighbor search (k maybe 3â€“8).
   * Optionally filter by metadata (e.g., only this userâ€™s memories, or only documents from last year).

6. **Rerank / score**

   * Optionally run a reranker (cross-encoder) that scores candidate chunks with more compute for quality.

7. **Inject into prompt**

   * Insert retrieved chunks or an extracted summary into the LLM prompt (labeled as â€œRETRIEVED CONTEXTâ€ and include metadata to cite).

8. **Decide writeback**

   * Optionally store new facts from the conversation back into the vector DB.

---

### Chunking details â€” why it matters

* Very small chunks â†’ retrieval is precise but loses surrounding context.
* Very large chunks â†’ may contain irrelevant content and waste tokens.
* Overlap helps preserve continuity across chunk boundaries.

A good default: chunk size **\~300â€“500 tokens** with **50â€“150 token overlap**. Tune by domain.

---

### Similarity & retrieval

* **Similarity metrics:** cosine similarity (most common) or dot product. Normalizing vectors makes cosine = dot product on normalized vectors.
* **ANN (Approximate Nearest Neighbors):** at scale, you use ANN indexes (HNSW, IVF, PQ) to get fast results with small recall loss. Most vector DBs handle this for you.

---

### Metadata & filtering â€” why store it

Store `user_id`, `source_type`, `timestamp`, `chunk_id`, `confidentiality` flags. Metadata lets you:

* Limit retrieval to a userâ€™s own memories.
* Exclude documents marked sensitive.
* Prefer recent documents using time-based weighting.

---

### Writing policy â€” what to store and when

You must decide when to persist something into long-term memory. Options:

* **Always store everything:** simple but wasteful and privacy risky.
* **Selective rules:** store only when text matches patterns or classifier says â€œimportantâ€ (e.g., â€œMy birthday is May 12â€, â€œMy address is Xâ€, or "I prefer vegan meals").
* **Human-in-the-loop:** ask user for permission: â€œDo you want me to remember that?â€
* **Automatic importance scoring:** run a small classifier that tags messages as `preference`, `fact`, `goal`, `transient`.

**Storage format idea:**

```json
{
  "chunk_id": "c123",
  "text": "Arjun prefers Python and short answers.",
  "embedding": [0.001, -0.234, ...],
  "metadata": {"user_id":"u42","type":"preference","ts":"2025-08-10T09:00Z"}
}
```

---

### Memory deduplication & merging

Avoid storing the same fact repeatedly. Use similarity checks when ingesting: if a new chunk is very close to an existing chunk (cosine similarity > threshold, e.g., 0.95), either skip or merge metadata (update timestamp).

---

### Eviction / retention / TTL strategies

* **Time-based TTL:** delete after N days unless marked important.
* **LRU / importance score:** evict least used or least important memories when storage limits hit.
* **Quota per user:** cap memory size per user to avoid abuse.
* **Archival & reindexing:** periodically re-embed old content if embedding model updates.

---

### Privacy & compliance

* Encrypt embeddings and metadata at rest.
* Allow data deletion and export.
* Store consent logs.
* Consider processing sensitive fields (PII) specially: redact, hash, or avoid storage unless necessary.

---

### RAG â€” how to combine long-term memory with short-term memory

Retrieval-Augmented Generation uses retrieval results to **augment** the short-term prompt.

**Process example (end-to-end):**

1. Receive user query â†’ update session (short-term).
2. Embed query â†’ query vector DB â†’ get top-k chunks.
3. Build final prompt:

   ```
   System: you are...
   Short-term history: ...
   Retrieved facts (labeled): [chunk 1 text], [chunk 2 text] (include source ids)
   User: [current query]
   ```
4. Tell the LLM: â€œUse the retrieved facts to answer. If facts contradict, prefer retrieved facts.â€
5. LLM generates answer and optionally cites chunk\_ids.

This reduces hallucination and gives you traceability.

---

# Practical implementation recipes (copy/paste style) ðŸ§©

### A. Short-term session management (Node + Redis pseudocode)

```js
// Node/Express + ioredis
const redis = new Redis();

async function addUserMessage(sessionId, msg) {
  // store as JSON string in list
  await redis.rpush(`sess:${sessionId}:history`, JSON.stringify(msg));
  // keep only last 20 messages
  await redis.ltrim(`sess:${sessionId}:history`, -20, -1);
  await redis.expire(`sess:${sessionId}:history`, 24 * 3600); // TTL 24h
}

async function getSessionHistory(sessionId) {
  const raw = await redis.lrange(`sess:${sessionId}:history`, 0, -1);
  return raw.map(JSON.parse);
}
```

**Notes:** use `ltrim` to keep last N turns. If you use token budget, calculate tokens and pop older items until under budget.

---

### B. Simple summarization loop (pseudo)

```py
# Python-style pseudocode
if session_token_count(session) > MAX_TOKENS:
    oldest_chunk = pop_oldest_messages(session, n=6)
    summary = call_llm_summarizer(oldest_chunk)  # low temp, specific instructions
    save_summary(session, summary)
    # replace oldest_chunk with summary in session history
```

Make sure summarizer output is deterministic-ish (low temperature).

---

### C. Long-term memory ingest + retrieval (Python-like pseudocode)

```py
# 1. Chunking
chunks = chunk_text(document_text, chunk_size=400, overlap=100)

# 2. Embed (batch)
embeddings = embedding_model.embed_batch([c.text for c in chunks])

# 3. Store
for chunk, vec in zip(chunks, embeddings):
    vector_db.upsert(id=chunk.id, vector=vec, metadata=chunk.metadata)

# Retrieval
q_vec = embedding_model.embed(query)
candidates = vector_db.query(q_vec, top_k=5, filter={"user_id": user_id})
```

For production, batch embeddings to reduce cost and parallelize.

---

### D. Prompt builder pattern (safe & structured)

```text
SYSTEM: You are an assistant that must only use verified facts.
CONTEXT â€” USER PROFILE:
- favorite_language: Python
- likes_short_answers: true

RECENT CHAT:
User: ...
Assistant: ...

RETRIEVED DOCS (IDs: d13,d77):
[Doc d13 summary or excerpt]
[Doc d77 summary or excerpt]

USER QUESTION:
...
```

Ask the LLM to `answer and list the source IDs used`.

---

# Hyperparameters & quick defaults (start here, tune later) âš™ï¸

* Short-term sliding window: **6â€“10 turns**.
* Token budget for prompt (excluding system): **3000 tokens** (adjust per model).
* Chunk size for long-term: **300â€“500 tokens**.
* Chunk overlap: **50â€“150 tokens**.
* Topâ€‘k retrieval: **3â€“8**.
* Similarity threshold for dedup: **>0.95** skip/merge.
* Summarization style: one-sentence bullet for personal preferences, paragraph for long context.

---

# Common failure modes & how to fix them (hands-on) ðŸ”§

1. **Model forgets earlier facts (context overflow)**
   *Fix:* Summarize older parts into short facts; move important facts to long-term memory.

2. **Model hallucinates facts**
   *Fix:* Use RAG and force citations. Add a final verification step (ask model â€œAre you sure?â€ then check sources).

3. **Irrelevant retrieved chunks (noise)**
   *Fix:* Add reranker or metadata filters, reduce top\_k, improve chunking.

4. **PII leakage**
   *Fix:* Classify & redact PII, encrypt storage, provide deletion endpoints.

5. **Slow response time**
   *Fix:* Cache frequent retrievals, batch embeddings, pre-compute common queries, reduce retrieved chunks.

---

# Debugging & monitoring â€” essential signals to log ðŸ“Š

* Which `chunk_ids` were retrieved for each query.
* Similarity scores for top candidates.
* Prompt token counts (input + output).
* LLM response latency and errors.
* Summary updates and summarizer outputs.
* Writes into long-term DB (what and when).

These logs help you track relevance and diagnose hallucination sources.

---

# UX considerations (how to surface memory behavior to users) âœ¨

* **Transparency:** show â€œI remember that you like Pythonâ€ cards and give users the option to edit or delete.
* **Consent:** ask user â€œDo you want me to remember that?â€ for personal details.
* **Controls:** provide settings for memory depth (disable/enable longâ€‘term memory).
* **Explainability:** when using retrieved facts in replies, offer â€œshow sourcesâ€ link.

---

# Example full flow (end-to-end, illustrated)

```
User: "My name is Arjun. I like backend coding."
â†’ Short-term: store in session history.
â†’ Policy: detect 'preference' => create memory chunk "Arjun likes backend coding." ; embed & index.

Later:
User: "Any tips for my backend learning?"
â†’ Backend: embed query â†’ retrieve "Arjun likes backend coding" + relevant docs â†’ build prompt with system + recent history + retrieved memory â†’ LLM generates answer tailored to backend learning and references the memory.
```

---

# Final checklist for implementing memory in your bot âœ…

* [ ] Use system prompt for rules/persona.
* [ ] Keep short-term session with sliding window + token budgeting.
* [ ] Implement summarization (incremental) for older chat.
* [ ] Build long-term memory pipeline: chunk â†’ embed â†’ index â†’ retrieve â†’ inject.
* [ ] Decide writeback policy (rules, classifier, or confirmation).
* [ ] Add metadata to every vector (user, ts, type).
* [ ] Put privacy & deletion APIs in place.
* [ ] Instrument retrievals, sources, and token usage for debugging.

---